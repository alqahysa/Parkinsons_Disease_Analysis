---
title: "03-Regression"
author: "Sarah AlQahtani"
date: "11/10/2020"
output: html_document
---
```{r}
library(glmnet)
```

Using the same dataset leveraged for Portfolio Builder Exercise #1, write up a second report that answers the following:

1. Depending on the type of response variable, apply a linear or logistic regression model.

   As the target variable in the dataset is continues _total_UPDRS_, we are going to apply Liner Regression model.

 - First, apply the model to your data without pre-applying feature engineering processes.
```{r}
set.seed(123)  # for reproducibility
(cv_model1 <- train(
  form = total_UPDRS ~ ., 
  data = par_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```
 
 - Create and a apply a blueprint of feature engineering processes that you think will help your model improve.
```{r}
blueprint
```
 
 - Now reapply the model to your data that has been feature engineered.
```{r}
set.seed(123)  # for reproducibility
(cv_model2 <- train(
  blueprint, 
  data = par_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```
 - Did your model performance improve?
  Yes the performance has improved after applying features engineering. Since the RMSE in the first model was `r cv_model1$results %>% filter(k == cv_model1$bestTune$k) %>% pull(RMSE) %>% scales::comma()`. However, it decreased after preprocess the features to become `r cv_model2$results %>% filter(k == cv_model2$bestTune$k) %>% pull(RMSE) %>% scales::comma()`. 
2. Apply a principal component regression model.
 - Perform a grid search over several components.
 - Identify and explain the performance of the optimal model.
```{r}
set.seed(123)
cv_model_pcr <- train(
  blueprint, 
  data = par_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 100
  )

# model with lowest RMSE
cv_model_pcr$bestTune
```
```{r}
cv_model_pcr$results %>%
  dplyr::filter(ncomp == pull(cv_model_pcr$bestTune))
```
```{r}
ggplot(cv_model_pcr)
```
 
3. Apply a partial least squares regression model.
 - Perform a grid search over several components.
 - Identify and explain the performance of the optimal model.
```{r}
set.seed(123)
cv_model_pls <- train(
  blueprint, 
  data = par_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 100
  )

# model with lowest RMSE
cv_model_pls$bestTune
```
```{r}
cv_model_pls$results %>%
  dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
```
```{r}
ggplot(cv_model_pls)

```

 
4. Apply a regularized regression 
```{r}
# we use model.matrix(...)[, -1] to discard the intercept
X <- model.matrix(total_UPDRS ~ ., par_train)[, -1]

# Y is normally distributed so we don't have to trasform it
Y <- par_train$total_UPDRS
```

 - Perform a grid search across alpha parameter values ranging between 0â€“1.
```{r}
# for reproducibility
set.seed(123)

# grid search across 
cv_glmnet <- train(
  x = X,
  y = Y,
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)
```
 
 - What is the optimal alpha and lambda values?
```{r}
cv_glmnet$bestTune
```
 
 - What is the MSE and RMSE for this optimal model?
```{r}
cv_glmnet$results %>%
  filter(alpha == cv_glmnet$bestTune$alpha, lambda == cv_glmnet$bestTune$lambda)
```
 - How does it compare to your previous models?
5. Pick the best performing model from above.
 - Identify the most influential features for this model.
 - Plot the top 10 most influential features.
```{r}
# predict sales price on training data
pred <- predict(cv_glmnet, X)

# compute RMSE of transformed predicted
RMSE(pred, Y)
```
 
```{r}
vip::vip(cv_glmnet, num_features = 10, method = "model")
```
 - Do these features have positive or negative impacts on your response variable?
```{r}
p1 <- pdp::partial(cv_glmnet, pred.var = "motor_UPDRS", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(motor_UPDRS, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000))

p2 <- pdp::partial(cv_glmnet, pred.var = "Shimmer.APQ3", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Shimmer.APQ3, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000))

p3 <- pdp::partial(cv_glmnet, pred.var = "Shimmer.APQ11") %>%
  mutate(
    yhat = exp(yhat),
    Shimmer.APQ11 = factor(Shimmer.APQ11)
    ) %>%
  ggplot(aes(Shimmer.APQ11, yhat)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 300000))

p4 <- pdp::partial(cv_glmnet, pred.var = "Jitter", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Jitter, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000))

grid.arrange(p1, p2, p3, p4, nrow = 2)
```
 

